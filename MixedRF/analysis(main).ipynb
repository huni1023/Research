{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학업탄력성 영향요인 연구\n",
    "@author: sjh\n",
    "\n",
    "- 전체 데이터 로드 및 가공\n",
    "\n",
    "## 1. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Current OS:  win32\n",
      ">> Current WD:  c:\\Users\\jhun1\\Dev\\Research\\MixedRF\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sys import platform\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# unicode minus를 사용하지 않기 위한 설정 (minus 깨짐현상 방지)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "# 설치된 폰트 출력\n",
    "import matplotlib.font_manager as fm\n",
    "font_list = [font.name for font in fm.fontManager.ttflist]\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "print('>> Current OS: ', platform)\n",
    "print('>> Current WD: ', BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "- 앞선 처리 데이터 불러오기\n",
    "- Load.py로 처리함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedData = {'SK': [\n",
    "            pd.read_excel(os.path.join(BASE_DIR,'data', 'cleanedData(SK).xlsx'), sheet_name='stu'),\n",
    "            pd.read_excel(os.path.join(BASE_DIR,'data', 'cleanedData(SK).xlsx'), sheet_name='sch'),\n",
    "            pd.read_excel(os.path.join(BASE_DIR,'data', 'cleanedData(SK).xlsx'), sheet_name='tch'),\n",
    "            \n",
    "            ],\n",
    "        'US': [\n",
    "            pd.read_excel(os.path.join(BASE_DIR,'data', 'cleanedData(US).xlsx'), sheet_name='stu'),\n",
    "            pd.read_excel(os.path.join(BASE_DIR,'data', 'cleanedData(US).xlsx'), sheet_name='sch'),\n",
    "            pd.read_excel(os.path.join(BASE_DIR,'data', 'cleanedData(US).xlsx'), sheet_name='tch'),\n",
    "            ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, LoadedData, codeBook, dummyCodeBook, PV_var):\n",
    "        self.data = LoadedData\n",
    "        BASE_DIR = r'C:\\Users\\jhun1\\Dropbox\\[2]Project\\[혼합효과 랜덤포레스트_2022]'\n",
    "        self.cb = pd.read_excel(os.path.join(BASE_DIR, 'drive-download-20220816T053902Z-001', codeBook), sheet_name='변수선택(1213)')\n",
    "        self.PV_var = PV_var\n",
    "        with open(dummyCodeBook, encoding='utf-8') as json_file:\n",
    "            self.dummyCB = json.load(json_file)\n",
    "        self.testBook = {\n",
    "                    'read/math/sci_1': 'PV1MATH PV1READ PV1SCIE'.split(),\n",
    "                    'read/math_1': 'PV1MATH PV1READ'.split(),\n",
    "                    'read/math_10': 'PV1MATH PV2MATH PV3MATH PV4MATH PV5MATH PV6MATH PV7MATH PV8MATH PV9MATH PV10MATH PV1READ PV2READ PV3READ PV4READ PV5READ PV6READ PV7READ PV8READ PV9READ PV10READ'.split(),\n",
    "                    'read_1': ['PV1READ'],\n",
    "                    'read_10': 'PV1READ PV2READ PV3READ PV4READ PV5READ PV6READ PV7READ PV8READ PV9READ PV10READ'.split()\n",
    "                }\n",
    "        self.nation_real_name = {'SK': '대한민국', 'US': '미국'} \n",
    "\n",
    "        self.valid_data = {}\n",
    "        self._1_dummy = {}\n",
    "        self._2_joined = {}\n",
    "        self._3_dropNa = {}\n",
    "        self._4_ESCS = {'full': {}, 'sliced': {}} # 여기서 데이터 갈라야함\n",
    "        self._5_shouldBeCal = {}\n",
    "        self.finalRS = {}\n",
    "        \n",
    "        self.rs_1_columnFull = pd.DataFrame()\n",
    "        self.rs_1_columnSK = pd.DataFrame()\n",
    "        self.rs_1_columnUS = pd.DataFrame()\n",
    "        \n",
    "\n",
    "    ### Needs1. is selected variable contained in both dataset\n",
    "    def noDataColumn(self): # 열별로 계산\n",
    "        toDrop = {}\n",
    "        \n",
    "        for nationName, nationalData in self.data.items():\n",
    "\n",
    "            toDrop[nationName] = []\n",
    "            for idx, (label, inputDf) in enumerate(zip('stu sch tch'.split(), nationalData)):\n",
    "                if label == 'tch':\n",
    "                    continue\n",
    "                else:\n",
    "                    # print(label)\n",
    "                    for column in inputDf.columns:\n",
    "                        if inputDf[column].isna().sum() > (inputDf.shape[0] * 0.8):\n",
    "                            # print('>>> over 80% is NA: ', column)\n",
    "                            toDrop[nationName].append(column)\n",
    "                        \n",
    "                        elif 'missing' in inputDf[column].values:\n",
    "                            # print('>>> missing: ', column)\n",
    "                            toDrop[nationName].append(column)\n",
    "                        \n",
    "                        else:\n",
    "                            continue\n",
    "        if (len(toDrop['SK']) ==0) and (len(toDrop['US']) ==0):\n",
    "            self.valid_data = self.data\n",
    "        else:\n",
    "            # raise ValueError(\"NaN column exist, check for this\")\n",
    "            before = self.data['SK'][0].shape[1]\n",
    "            for nation, groups in self.data.items():\n",
    "                self.valid_data[nation] = []\n",
    "                for idx, data in enumerate(groups):\n",
    "                    if idx == 0 :\n",
    "                        self.valid_data[nation].append(data.drop('PERSPECT', axis=1))\n",
    "                    else: \n",
    "                        self.valid_data[nation].append(data)\n",
    "\n",
    "            after = self.valid_data['SK'][0].shape[1]\n",
    "            assert before - after == 1, print(after, before)\n",
    "\n",
    "        return toDrop\n",
    "            # assert len(toDrop[nationName]) == 2, print(toDrop)\n",
    "\n",
    "\n",
    "    def Dummy(self, doDummy):\n",
    "        # match key and value from codeBook\n",
    "        # print('\\n\\n>>>> 1. Dummy coding')\n",
    "        def matchKV(codeBookDict, inputList):\n",
    "            outputLS = []\n",
    "            for val in inputList:\n",
    "                try:\n",
    "                    outputLS.append(codeBookDict[val])\n",
    "                except KeyError:\n",
    "                    outputLS.append(np.nan)\n",
    "            \n",
    "            return outputLS\n",
    "        \n",
    "        if doDummy == True:\n",
    "            notDummyCol1 = self.cb[self.cb['categories'] == 'identifier'].index\n",
    "            notDummyCol2 = self.cb[self.cb['categories'] == 'resilient status'].index\n",
    "            notDummyCol3 = self.cb[self.cb['file name'] == 'should be caculated'].index\n",
    "            \n",
    "            toDummy = self.cb.drop(list(notDummyCol1)+list(notDummyCol2)+list(notDummyCol3), axis=0) # 더미 변환 안할 변수 행 삭제함\n",
    "            # display(toDummy)\n",
    "\n",
    "            for nationalName, inputNational in self.valid_data.items():\n",
    "                outputNational = copy.deepcopy(inputNational)\n",
    "\n",
    "                for idx, row in toDummy.iterrows(): # 변수별로 반복문\n",
    "                    variable = row['NAME']\n",
    "                    \n",
    "                    if type(row['file name']) != str: # 분석에서 제외할 변수가 있어서 버림\n",
    "                        continue\n",
    "\n",
    "                    else:\n",
    "                        if ('STU' in row['file name']) and (variable in self.dummyCB['stu']):\n",
    "                            outputLS = matchKV(self.dummyCB['stu'][variable], outputNational[0][variable])\n",
    "                            outputNational[0][variable] = outputLS \n",
    "\n",
    "                    # 학교, 교사 데이터는 더미코딩할 것 없음\n",
    "                self._1_dummy[nationalName] = outputNational\n",
    "        elif doDummy == False:\n",
    "            self._1_dummy = copy.deepcopy(self.valid_data)\n",
    "        else:\n",
    "            raise TypeError('>> Error: check option Type')\n",
    "\n",
    "\n",
    "    def Join(self):\n",
    "        # print('\\n\\n>>>> 2. Join DataFrame')\n",
    "\n",
    "        for nationalName, inputNational in self._1_dummy.items():\n",
    "            # print('>> join nation: ', nationalName)\n",
    "            inputNational[0].reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            outputDf = copy.deepcopy(inputNational[0])\n",
    "            # print('>> before ', outputDf.shape)\n",
    "            before = outputDf.shape\n",
    "\n",
    "\n",
    "            inputNational[1].drop(['CNTRYID', 'CNT'], axis=1, inplace=True)\n",
    "            if inputNational[1].index.name == 'CNTSCHID':\n",
    "                pass\n",
    "            else:\n",
    "                inputNational[1].set_index('CNTSCHID', drop=True, inplace=True)\n",
    "            \n",
    "            if inputNational[1].shape[1] == 0:\n",
    "                # print('>> school data is empty')\n",
    "                pass\n",
    "            else:\n",
    "                for idx, row in tqdm(outputDf.iterrows(), desc=\">> mapping\"):\n",
    "                    toBeInput = inputNational[1].loc[row['CNTSCHID']].values # 학생 데이터에 들어가야할 학교 데이터 찾기\n",
    "                    assert len(toBeInput) == inputNational[1].shape[1]\n",
    "                    \n",
    "                    toBeInput_T = toBeInput.reshape(1, 8)\n",
    "                    outputDf.loc[idx, list(inputNational[1].columns)] = toBeInput_T[0]\n",
    "            \n",
    "                after = outputDf.shape\n",
    "                print('>>>> Bef: ', before, '....', 'Aft: ', after)\n",
    "                assert 'EDUSHORT' in outputDf.columns\n",
    "\n",
    "            self._2_joined[nationalName] = outputDf\n",
    "\n",
    "    def DropStudent(self, isVisualize=True):\n",
    "        # 각 column 별로 데이터 검수\n",
    "        # print('\\n>>>> 3. Verify na and Drop student')\n",
    "        # print(self._2_joined.keys())\n",
    "        def column_wise(inputData):\n",
    "            if type(inputData) == dict:\n",
    "                merged = pd.concat([inputData['SK'], inputData['US']])\n",
    "                assert merged.shape[0] == inputData['SK'].shape[0] + inputData['US'].shape[0]\n",
    "            elif type(inputData) == pd.DataFrame:\n",
    "                merged = copy.deepcopy(inputData)\n",
    "            \n",
    "            else:\n",
    "                raise TypeError('>> Error: Check your input D type')\n",
    "                \n",
    "\n",
    "            describeDF = merged.describe().T\n",
    "            describeDF['NA_ratio'] = round(\n",
    "                100 - describeDF['count']/merged.shape[0]*100,\n",
    "                 2\n",
    "                 )\n",
    "\n",
    "            newColumnOrder = [describeDF.columns[0], 'NA_ratio'] + list(describeDF.columns[1:-1])\n",
    "            describeDF= describeDF[newColumnOrder]\n",
    "            return describeDF\n",
    "\n",
    "        # 각 학생별로 데이터 검수\n",
    "        def row_wise(inputData):\n",
    "            merged = pd.concat([inputData['SK'], inputData['US']])\n",
    "            assert merged.shape[0] == inputData['SK'].shape[0] + inputData['US'].shape[0]\n",
    "            # unlike column wise, we prepare data with \n",
    "\n",
    "            for_histogram = {}\n",
    "            for label, data in zip(['full', 'SK', 'US'], [merged, inputData['SK'], inputData['US']]):\n",
    "                for_histogram[label] = []\n",
    "\n",
    "                for i in range(len(data.index)) :\n",
    "                    na_ratio = round((data.iloc[i].isnull().sum()/data.shape[1]) * 100, 0)\n",
    "                    for_histogram[label].append(na_ratio)\n",
    "\n",
    "            if isVisualize == True:\n",
    "                fig = plt.figure(figsize=(17,6))\n",
    "\n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.hist(for_histogram['full'])\n",
    "                plt.title('\\n전체 데이터\\n')\n",
    "                plt.xlabel('\\n전체 변수 대비 결측비율(%)\\n')\n",
    "                plt.ylabel('빈도')\n",
    "                \n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.hist(for_histogram['SK'])\n",
    "                plt.title('\\nSouth Korea\\n')\n",
    "                plt.xlabel('\\n전체 변수 대비 결측비율(%)\\n')\n",
    "                plt.ylabel('빈도')\n",
    "                \n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.hist(for_histogram['US'])\n",
    "                plt.title('\\nUnited States\\n')\n",
    "                plt.xlabel('\\n전체 변수 대비 결측비율(%)\\n')\n",
    "                plt.ylabel('빈도')\n",
    "\n",
    "                plt.savefig(os.path.join(BASE_DIR, 'data', f'NA_ratio.jpg'))\n",
    "                plt.show()\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            return for_histogram\n",
    "        \n",
    "        def dropOver(inputData, rowWiseResult):\n",
    "            assert type(rowWiseResult) == list\n",
    "            output = copy.deepcopy(inputData)\n",
    "\n",
    "            toDrop = []\n",
    "            for idx, sumNA in zip(output.index, rowWiseResult):\n",
    "                if sumNA > 30:\n",
    "                    toDrop.append(idx)\n",
    "            \n",
    "            before = output.shape[0]\n",
    "            output.drop(toDrop, axis=0, inplace=True)\n",
    "            after = output.shape[0]\n",
    "            # print('>> NA drop: ', before - after)\n",
    "\n",
    "            return output\n",
    "        \n",
    "        self.rs_1_columnFull = column_wise(self._2_joined)\n",
    "        self.rs_1_columnSK = column_wise(self._2_joined['SK'])\n",
    "        self.rs_1_columnUS = column_wise(self._2_joined['US'])\n",
    "\n",
    "        rowWiseNA = row_wise(self._2_joined)\n",
    "        self._3_dropNa['SK'] = dropOver(self._2_joined['SK'], rowWiseResult=rowWiseNA['SK'])\n",
    "        self._3_dropNa['US'] = dropOver(self._2_joined['US'], rowWiseResult=rowWiseNA['US'])\n",
    "        \n",
    "        \n",
    "    def ESCS(self, threshold, isVisualize=True):\n",
    "        r\"\"\"\n",
    "        데이터를 쪼개줌\n",
    "        full, slice\n",
    "        \n",
    "        1. PV_var: integer 1 ~ 10\n",
    "        arg1. test_key : if 'score_calculating_method'\n",
    "        arg2. Threshold_Reading_Score : score value\n",
    "        4. isVisualize: if true, visualization proceed\n",
    "        \"\"\"\n",
    "        print('\\n>>>> 4. Slicing data by ESCS')\n",
    "        \n",
    "        def thresholdCalculator(inputData,\n",
    "                                PV_var,\n",
    "                                threshold):\n",
    "            r\"\"\"학업탄력성을 판별하는 기준값을 계산함\"\"\"\n",
    "            assert type(PV_var) == int, print('>> Error__PV_var: ', PV_var)\n",
    "            assert PV_var > 0, print('>> Error__PV_var: ', PV_var)\n",
    "            assert PV_var < 11, print('>> Error__PV_var: ', PV_var)\n",
    "\n",
    "\n",
    "            threshold_dict = {'SK': {}, 'US': {}}\n",
    "\n",
    "            for nationalName, inputNational in inputData.items():\n",
    "                # cal academic score\n",
    "                targetColumn = ['PV'+ str(PV_var) + 'READ']\n",
    "                inputNational['AcademicScore'] = inputNational.loc[:, targetColumn].mean(axis=1)\n",
    "                \n",
    "                assert type(threshold) == int, print('Insert validate type args : ', threshold)\n",
    "                threshold_dict[nationalName]['academic_score'] = threshold\n",
    "                \n",
    "                # cal escs score\n",
    "                threshold_dict[nationalName]['escs_score'] = inputNational['ESCS'].quantile(0.25)\n",
    "\n",
    "            return threshold_dict, inputData # 새로운 열이 추가되었으므로 리턴해서 사용해야함\n",
    "        \n",
    "        def escsSlice(inputDict, escsThreshold):\n",
    "            r\"\"\"계산하기\"\"\"\n",
    "            assert type(inputDict) == dict, print('>> Error: must input Dict')\n",
    "            assert type(escsThreshold) == dict\n",
    "            output = {'SK': pd.DataFrame(), 'US': pd.DataFrame()}\n",
    "            for nationalName, inputNational in inputDict.items():\n",
    "                \n",
    "                before = inputNational.shape[0]\n",
    "                toDrop = []\n",
    "                for idx, val in zip(inputNational['ESCS'].index, inputNational['ESCS'].values):\n",
    "                    if val < escsThreshold[nationalName]['escs_score']:\n",
    "                        continue\n",
    "                    else:\n",
    "                        toDrop.append(idx) # escs 하위 25%를 넘는 친구들은 버림\n",
    "                \n",
    "                \n",
    "                output[nationalName] = inputNational.drop(toDrop, axis=0)\n",
    "                after = output[nationalName].shape[0]\n",
    "                # print('>> before: ', before, '>> after: ', after)\n",
    "            \n",
    "            return output\n",
    "\n",
    "\n",
    "        def quantileCalculator( \n",
    "                            inputData, # 전체 Full, escs 하위 25%로 데이터셋이 2개로 나뉘므로 인풋을 줘야함\n",
    "                            option, # full: 전체 데이터, sliced: 잘린 데이터\n",
    "                            AcademicThreshold\n",
    "                            ):\n",
    "            \n",
    "            assert type(AcademicThreshold) == dict\n",
    "\n",
    "            output = {'SK': pd.DataFrame(), 'US': pd.DataFrame()}\n",
    "            count_ratio = {'SK': [], 'US': []}\n",
    "            for IDX, (nationalName, inputNational) in enumerate(inputData.items()):\n",
    "                total = inputNational.shape[0]\n",
    "                \n",
    "                iamResilient = []\n",
    "                if option == 'full':\n",
    "                    escsVar = inputNational['ESCS'].quantile(0.25) # 하위 25%\n",
    "                    for idx, row in inputNational.iterrows():\n",
    "                        if row['AcademicScore'] > AcademicThreshold[nationalName]['academic_score'] and row['ESCS'] < AcademicThreshold[nationalName]['escs_score']: # sliced 데이터에서는 이 기준을 만족할 수 없음\n",
    "                            iamResilient.append(1)\n",
    "                        else:\n",
    "                            iamResilient.append(0)\n",
    "\n",
    "                elif option == 'sliced':\n",
    "                    for idx, row in inputNational.iterrows():\n",
    "                        if row['AcademicScore'] > AcademicThreshold[nationalName]['academic_score']: # sliced 데이터는 escs 기준 필요 없음\n",
    "                            iamResilient.append(1)\n",
    "                        else:\n",
    "                            iamResilient.append(0)\n",
    "\n",
    "                inputNational['resilient'] = iamResilient\n",
    "                resilientCount = [x for x in iamResilient if x ==1]\n",
    "                resilientRatio = round(len(resilientCount)/total*100, 2)\n",
    "                # print(f'>> 회복탄력성 학생수({nationalName}): ', len(resilientCount), f'({resilientRatio})%')\n",
    "\n",
    "                output[nationalName] = inputNational\n",
    "                count_ratio[nationalName].append(len(resilientCount))\n",
    "                count_ratio[nationalName].append(resilientRatio)\n",
    "\n",
    "            return output, count_ratio\n",
    "\n",
    "        def visualize(inputData,\n",
    "                    option, # full: 전체 데이터, sliced: 잘린 데이터\n",
    "                    figName, # 그림 제목\n",
    "                    AcademicThreshold\n",
    "                        ):\n",
    "\n",
    "            fig = plt.figure(figsize=(17,9))\n",
    "            for IDX, (nationalName, inputNational) in enumerate(inputData.items()):\n",
    "\n",
    "                plt.subplot(2, 2, 2*IDX+1)\n",
    "                plt.hist(inputNational['AcademicScore'])\n",
    "                plt.title(f'\\n학업성취{self.nation_real_name[nationalName]}\\n')\n",
    "                plt.xlabel('\\n점수\\n')\n",
    "                plt.axvline(AcademicThreshold[nationalName]['academic_score'], color='r', linewidth=1, linestyle='--')\n",
    "                \n",
    "                plt.subplot(2, 2, 2*IDX+2)\n",
    "                plt.hist(inputNational['ESCS'])\n",
    "                plt.title(f'\\n사회문화경제{self.nation_real_name[nationalName]}\\n')\n",
    "                plt.xlabel('\\n점수\\n')\n",
    "                if option=='full':\n",
    "                    plt.axvline(AcademicThreshold[nationalName]['escs_score'], color='r', linewidth=1, linestyle='--')\n",
    "\n",
    "                \n",
    "            plt.savefig(os.path.join(BASE_DIR, 'rs', f'{figName}_{option}.jpg'))\n",
    "            plt.show()\n",
    "        \n",
    "        ## 1. calculate threshold value\n",
    "        AcademicThreshold, newData_dict = thresholdCalculator(self._3_dropNa,\n",
    "                                                            PV_var = self.PV_var,\n",
    "                                                            threshold=threshold) ## 학업성취 코딩 방법을 바꿀 때 여기 arg를 조정\n",
    "        \n",
    "        \n",
    "        ## 2. slice\n",
    "        self._4_ESCS['full'] = copy.deepcopy(newData_dict) # no drop case, so just copied\n",
    "        self._4_ESCS['sliced'] = escsSlice(newData_dict, escsThreshold = AcademicThreshold)\n",
    "        assert type(self._4_ESCS['full']) == dict, print(self._4_ESCS['full'])\n",
    "\n",
    "\n",
    "        ## 3. labeling resilient student\n",
    "        self._4_ESCS['full'], temp = quantileCalculator(inputData=self._4_ESCS['full'], \n",
    "                                                option = 'full',\n",
    "                                                AcademicThreshold= AcademicThreshold)\n",
    "        self._4_ESCS['sliced'], resilientCount_Ratio = quantileCalculator(inputData=self._4_ESCS['sliced'], \n",
    "                                                option = 'sliced',\n",
    "                                                AcademicThreshold= AcademicThreshold)\n",
    "\n",
    "        ## 4. visualize resilient student\n",
    "        if isVisualize == True:\n",
    "            visualize(self._4_ESCS['full'], option='full', figName='읽10', AcademicThreshold= AcademicThreshold)\n",
    "            visualize(self._4_ESCS['sliced'], option = 'sliced', figName ='읽10(target paper)', AcademicThreshold= AcademicThreshold)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return resilientCount_Ratio\n",
    "\n",
    "    \n",
    "    # should be calculated 변수들 계산하는 것임\n",
    "    def shouldBeCalculated(self):\n",
    "        # print('\\n\\n>>>> 6. Should Be Calculated')\n",
    "        \n",
    "        def schoolMean(inputDf, whichVar):\n",
    "            assert type(whichVar) == list\n",
    "            outputMean = {}\n",
    "            for sch_id in inputDf['CNTSCHID'].values:\n",
    "                if sch_id in outputMean.keys():\n",
    "                    continue\n",
    "                \n",
    "                else:\n",
    "                    temp1 = inputDf[inputDf['CNTSCHID'] == sch_id]\n",
    "                    temp2 = temp1.loc[:, whichVar] \n",
    "                    assert len(temp2.columns) == len(whichVar)\n",
    "                    meanVal = np.nanmean(temp2.values)\n",
    "                    assert type(meanVal) == np.float64, print('Error : ', type(meanVal))\n",
    "\n",
    "                    outputMean[sch_id] = meanVal\n",
    "            \n",
    "            return outputMean\n",
    "        \n",
    "        \n",
    "        def meanMapping(inputColumn, mean_dict):\n",
    "            outputLS = []\n",
    "            for idx, sch_id in enumerate(inputColumn.values):\n",
    "                outputLS.append(mean_dict[sch_id])\n",
    "\n",
    "            return outputLS\n",
    "        \n",
    "\n",
    "        def matching(inputData, codeBook):\n",
    "            output = copy.deepcopy(inputData)\n",
    "            shouldBeCal = self.cb[self.cb['file name'] == 'should be caculated']\n",
    "            assert len(shouldBeCal) == 2, print('Error: check self.cb')\n",
    "            for national in output.keys():\n",
    "                beforeShape = output[national].shape[1]\n",
    "                \n",
    "                calVal = list(codeBook[codeBook['categories'] == 'resilient status']['NAME'])\n",
    "                calVal.remove('ESCS')\n",
    "                \n",
    "                for variable in shouldBeCal['NAME'].values:\n",
    "                \n",
    "                    if variable == 'AVG_S_TEST':    \n",
    "                        mean_dict = schoolMean(output[national], calVal)\n",
    "                        \n",
    "                    elif variable == 'AVG_S_ESCS':\n",
    "                        mean_dict = schoolMean(output[national], ['ESCS'])\n",
    "\n",
    "                    #평균 dict 활용해서 매칭 진행\n",
    "                    outputLS = meanMapping(output[national]['CNTSCHID'], mean_dict)\n",
    "                    assert len(outputLS) == output[national].shape[0], print('Error: ', len(outputLS))\n",
    "                    output[national][variable] = outputLS # 학교 데이터이므로, 학교에 맞춰서 추가하기\n",
    "\n",
    "                afterShape = output[national].shape[1]\n",
    "                assert afterShape - beforeShape == 2, print('Beofre: ', beforeShape, ' ... ', 'After: ', afterShape)\n",
    "            \n",
    "            return output\n",
    "        \n",
    "        if 'should be calculated' in self.cb['file name'].values:\n",
    "            self._5_shouldBeCal['full'] = matching(self._4_ESCS['full'], codeBook = self.cb)\n",
    "            self._5_shouldBeCal['sliced'] = matching(self._4_ESCS['sliced'], codeBook = self.cb)\n",
    "        else:\n",
    "            self._5_shouldBeCal['full'] = copy.deepcopy(self._4_ESCS['full'])\n",
    "            self._5_shouldBeCal['sliced'] = copy.deepcopy(self._4_ESCS['sliced'])\n",
    "            \n",
    "\n",
    "    \n",
    "    def AdjustMinor(self):\n",
    "        r\"\"\"마이너한 것들을 조정하기 위함\"\"\"\n",
    "        \n",
    "        def Merge(inputData):\n",
    "            r\"\"\"두 데이터를 나라 row를 만들고, 합치기 위함\"\"\"\n",
    "            output = pd.concat([inputData['SK'], inputData['US']], axis=0)\n",
    "            assert inputData['SK'].shape[0] + inputData['US'].shape[0] == output.shape[0]\n",
    "\n",
    "            dropAcademic = ['CNTRYID', 'AcademicScore']\n",
    "            for column in output.columns:\n",
    "                if 'PV' in column:\n",
    "                    dropAcademic.append(column)\n",
    "\n",
    "            output.drop(dropAcademic, axis=1, inplace=True)\n",
    "            # print('>> columns: ', output.columns)\n",
    "            return output\n",
    "\n",
    "        def columnOrder(inputData,\n",
    "                        important_columns=['resilient']):\n",
    "            r\"\"\"spss 편하도록, 주요 변수들을 앞으로 빼는 작업\"\"\"\n",
    "            column_ID = ['CNT', 'CNTSCHID', 'CNTSTUID']\n",
    "\n",
    "            inputData.set_index(column_ID+important_columns, inplace=True)\n",
    "            inputData.reset_index(inplace=True)\n",
    "\n",
    "            return inputData\n",
    "\n",
    "        self.finalRS['full'] = Merge(self._5_shouldBeCal['full'])\n",
    "        self.finalRS['sliced'] = Merge(self._5_shouldBeCal['sliced'])\n",
    "\n",
    "        self.finalRS['full'] = columnOrder(self.finalRS['full'])\n",
    "        self.finalRS['sliced'] = columnOrder(self.finalRS['sliced'])\n",
    "\n",
    "    def Save(self):\n",
    "        with pd.ExcelWriter(os.path.join(BASE_DIR, 'rs', f'preprocessing{self.PV_var}.xlsx')) as writer:\n",
    "            self.finalRS['sliced'].to_excel(writer, sheet_name='sliced', index=False)\n",
    "\n",
    "\n",
    "# processor = Preprocessing(LoadedData=loadedData, codeBook=Loader.cb, dummyCodeBook='dummyCoding.json', PV_var=1)\n",
    "# processor.noDataColumn()\n",
    "# processor.Dummy(doDummy=False) # 굳이 dummy할 필요가 없음, rf에서 categorical / numerical 인식해야함\n",
    "# processor.Join()\n",
    "# processor.DropStudent(isVisualize=False)\n",
    "# processor.ESCS(480, isVisualize=False) # Lv4: 553, Lv3: 480, Lv2: 407\n",
    "# processor.shouldBeCalculated()\n",
    "# processor.AdjustMinor()\n",
    "# processor.Save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>  1\n",
      "\n",
      ">>>> 4. Slicing data by ESCS\n",
      ">  {'SK': [876, 52.96], 'US': [483, 41.14]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:04<00:41,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>  2\n",
      "\n",
      ">>>> 4. Slicing data by ESCS\n",
      ">  {'SK': [857, 51.81], 'US': [475, 40.46]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:09<00:36,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>  3\n",
      "\n",
      ">>>> 4. Slicing data by ESCS\n",
      ">  {'SK': [863, 52.18], 'US': [473, 40.29]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:13<00:31,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>  4\n",
      "\n",
      ">>>> 4. Slicing data by ESCS\n",
      ">  {'SK': [855, 51.69], 'US': [487, 41.48]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:18<00:26,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>  5\n",
      "\n",
      ">>>> 4. Slicing data by ESCS\n",
      ">  {'SK': [852, 51.51], 'US': [474, 40.37]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:22<00:22,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>  6\n",
      "\n",
      ">>>> 4. Slicing data by ESCS\n",
      ">  {'SK': [838, 50.67], 'US': [484, 41.23]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:27<00:17,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>  7\n",
      "\n",
      ">>>> 4. Slicing data by ESCS\n",
      ">  {'SK': [858, 51.87], 'US': [477, 40.63]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:31<00:13,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>  8\n",
      "\n",
      ">>>> 4. Slicing data by ESCS\n",
      ">  {'SK': [847, 51.21], 'US': [463, 39.44]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:35<00:08,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>  9\n",
      "\n",
      ">>>> 4. Slicing data by ESCS\n",
      ">  {'SK': [859, 51.93], 'US': [485, 41.31]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:40<00:04,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>  10\n",
      "\n",
      ">>>> 4. Slicing data by ESCS\n",
      ">  {'SK': [866, 52.36], 'US': [475, 40.46]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:44<00:00,  4.45s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>876</td>\n",
       "      <td>52.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>857</td>\n",
       "      <td>51.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>863</td>\n",
       "      <td>52.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>855</td>\n",
       "      <td>51.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>852</td>\n",
       "      <td>51.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>838</td>\n",
       "      <td>50.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>858</td>\n",
       "      <td>51.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>847</td>\n",
       "      <td>51.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>859</td>\n",
       "      <td>51.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>866</td>\n",
       "      <td>52.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  ratio\n",
       "1    876  52.96\n",
       "2    857  51.81\n",
       "3    863  52.18\n",
       "4    855  51.69\n",
       "5    852  51.51\n",
       "6    838  50.67\n",
       "7    858  51.87\n",
       "8    847  51.21\n",
       "9    859  51.93\n",
       "10   866  52.36"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sk = pd.DataFrame(index=range(1, 11), columns=['count', 'ratio'])\n",
    "plot_us = pd.DataFrame(index=range(1, 11), columns=['count', 'ratio'])\n",
    "\n",
    "for idx in tqdm(range(1, 11)):\n",
    "    print('>>>> ', idx)\n",
    "    processor = Preprocessing(LoadedData=loadedData, codeBook='PISA2018_CODEBOOK (변수선택-공유).xlsx', dummyCodeBook='dummyCoding.json', PV_var=idx)\n",
    "    processor.noDataColumn()\n",
    "    processor.Dummy(doDummy=False) # 굳이 dummy할 필요가 없음, rf에서 categorical / numerical 인식해야함\n",
    "    processor.Join()\n",
    "    processor.DropStudent(isVisualize=False)\n",
    "    resilientCount_Ratio = processor.ESCS(480, isVisualize=False) # Lv4: 553, Lv3: 480, Lv2: 407\n",
    "    print('> ', resilientCount_Ratio)\n",
    "    processor.shouldBeCalculated()\n",
    "    processor.AdjustMinor()\n",
    "    processor.Save()\n",
    "\n",
    "    plot_sk.loc[idx, 'count'] = resilientCount_Ratio['SK'][0]\n",
    "    plot_sk.loc[idx, 'ratio'] = resilientCount_Ratio['SK'][1]\n",
    "    plot_us.loc[idx, 'count'] = resilientCount_Ratio['US'][0]\n",
    "    plot_us.loc[idx, 'ratio'] = resilientCount_Ratio['US'][1]\n",
    "\n",
    "\n",
    "display(plot_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>483</td>\n",
       "      <td>41.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>475</td>\n",
       "      <td>40.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>473</td>\n",
       "      <td>40.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>487</td>\n",
       "      <td>41.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>474</td>\n",
       "      <td>40.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>484</td>\n",
       "      <td>41.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>477</td>\n",
       "      <td>40.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>463</td>\n",
       "      <td>39.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>485</td>\n",
       "      <td>41.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>475</td>\n",
       "      <td>40.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  ratio\n",
       "1    483  41.14\n",
       "2    475  40.46\n",
       "3    473  40.29\n",
       "4    487  41.48\n",
       "5    474  40.37\n",
       "6    484  41.23\n",
       "7    477  40.63\n",
       "8    463  39.44\n",
       "9    485  41.31\n",
       "10   475  40.46"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(plot_us)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.bar\n",
    "should be plotted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('khrrc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2ed4711497f95c801e11ec0b21dd929a4a0de8689951f49fbf9abba32131afd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
