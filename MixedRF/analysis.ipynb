{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학업탄력성 영향요인 연구\n",
    "@author: sjh\n",
    "\n",
    "## 1. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Current OS:  win32\n",
      ">> Current WD:  c:\\Users\\jhun1\\Proj\\Research\\MixedRF\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sys import platform\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# unicode minus를 사용하지 않기 위한 설정 (minus 깨짐현상 방지)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "# 설치된 폰트 출력\n",
    "import matplotlib.font_manager as fm\n",
    "font_list = [font.name for font in fm.fontManager.ttflist]\n",
    "if 'darwin' in platform:\n",
    "    plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "else:\n",
    "    plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "print('>> Current OS: ', platform)\n",
    "print('>> Current WD: ', BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Only Codebook will be loaded\n"
     ]
    }
   ],
   "source": [
    "class Load:\n",
    "    def __init__(self, stuFolder, schFolder, tchFolder, codeBook,\n",
    "                onlyCodeBook, ):\n",
    "        self.BASE_DIR = r'C:\\Users\\jhun1\\Dropbox\\[3]Project\\[혼합효과 랜덤포레스트_2022]\\PISA2018'\n",
    "\n",
    "        if onlyCodeBook == False:\n",
    "            print('>>>>> Init: load raw data')\n",
    "            stuFile = [FILE for FILE in os.listdir(os.path.join(self.BASE_DIR, stuFolder)) if FILE[-4:] == '.sav'][0]\n",
    "            schFile = [FILE for FILE in os.listdir(os.path.join(self.BASE_DIR, schFolder)) if FILE[-4:] == '.sav'][0]\n",
    "            tchFile = [FILE for FILE in os.listdir(os.path.join(self.BASE_DIR, tchFolder)) if FILE[-4:] == '.sav'][0]\n",
    "\n",
    "            self.rawStu = pd.read_spss(os.path.join(self.BASE_DIR, stuFolder, stuFile))\n",
    "            self.rawSCH = pd.read_spss(os.path.join(self.BASE_DIR, schFolder, schFile))\n",
    "            self.rawTCH = pd.read_spss(os.path.join(self.BASE_DIR, tchFolder, tchFile))\n",
    "            self.dataLS = [self.rawStu, self.rawSCH, self.rawTCH]\n",
    "\n",
    "            # desciptive\n",
    "            print('>> Stu data set', self.rawStu.shape)\n",
    "            print('>> Sch data set', self.rawSCH.shape)\n",
    "            print('>> Tch data set', self.rawTCH.shape)\n",
    "        \n",
    "        else:\n",
    "            print('>> Only Codebook will be loaded')\n",
    "            pass\n",
    "        \n",
    "        self.cb = pd.read_excel(codeBook)\n",
    "\n",
    "\n",
    "\n",
    "    def defaultCleaner(self):\n",
    "        print('\\n\\n>>>> Cleaning: default nation and variable')\n",
    "\n",
    "\n",
    "        ### SK, US dict 형태 잡아줌\n",
    "        def cleaningNational(dataLS, SouthKorea = 'Korea', US='United States'):\n",
    "            nationalData = {'SK': [], 'US': []}\n",
    "\n",
    "            for nation_name, code in zip(nationalData.keys(), [SouthKorea, US]):\n",
    "                print(f'\\n>> slicing: {nation_name}')\n",
    "                for data in dataLS:\n",
    "                    # print(data.head(5))\n",
    "                    temp2 = data[data['CNTRYID'] == code]\n",
    "                \n",
    "                    nationalData[nation_name].append(temp2)\n",
    "                    print('>> sliced shape: ', temp2.shape)\n",
    "            \n",
    "            return nationalData\n",
    "            \n",
    "\n",
    "        def cleaningVariable(data, codeBook):\n",
    "            output = {}\n",
    "            \n",
    "            for nation_name in data.keys():\n",
    "                Column_toSave = {'Stu': [], 'Sch': [], 'Tch': []}\n",
    "                for fileName, variable, category in tqdm(zip(codeBook['file name'].values, codeBook['NAME'].values, codeBook['categories']), desc=\">> variable check\"):\n",
    "                    # 코드북 내에서도 분석에서 제할 변수는 file name을 비움\n",
    "                    if type(fileName) != str:\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    else: \n",
    "                        if category == 'identifier':\n",
    "                            Column_toSave['Stu'].append(variable)\n",
    "                            \n",
    "                            if variable == 'CNTSTUID':\n",
    "                                continue\n",
    "                            else: # 학교, 교사 셋에는 해당 변수가 없어서 추가해줌\n",
    "                                Column_toSave['Sch'].append(variable)\n",
    "                                Column_toSave['Tch'].append(variable)\n",
    "\n",
    "                        else:\n",
    "                            if 'STU' in fileName:\n",
    "                                if variable in data[nation_name][0].columns:\n",
    "                                    Column_toSave['Stu'].append(variable)\n",
    "                                else:\n",
    "                                    print('>> none(stu)', variable)\n",
    "\n",
    "                            elif 'SCH' in fileName:\n",
    "                                if variable in data[nation_name][1].columns:\n",
    "                                    Column_toSave['Sch'].append(variable)\n",
    "                                else:\n",
    "                                    print('>> none(sch)', variable)\n",
    "\n",
    "                            elif 'TCH' in fileName:\n",
    "                                if variable in data[nation_name][2].columns:\n",
    "                                    Column_toSave['Tch'].append(variable)\n",
    "                                else:\n",
    "                                    print('>> none(tch)', variable)\n",
    "                    \n",
    "                \n",
    "                # print('>>>> save: ', Column_toSave)\n",
    "                output[nation_name] = [data[nation_name][0][Column_toSave['Stu']],\n",
    "                                        data[nation_name][1][Column_toSave['Sch']],\n",
    "                                        data[nation_name][2][Column_toSave['Tch']],\n",
    "                                        ]\n",
    "                assert len(Column_toSave['Tch']) < 4, print(Column_toSave['Tch'])\n",
    "\n",
    "            assert 'SK' in output.keys()\n",
    "            assert 'US' in output.keys()\n",
    "            return output\n",
    "\n",
    "        cleaned_Nation = cleaningNational(dataLS = self.dataLS)\n",
    "        self.default_cleaningData = cleaningVariable(data = cleaned_Nation, codeBook = self.cb)\n",
    "\n",
    "        # categories 에서 일단은 codebook에 있는 변수가 다 있나 확인\n",
    "        # categories 에서 individual & family, school 구분\n",
    "\n",
    "\n",
    "\n",
    "LOAD_ONLY_CODEBOOK = True\n",
    "\n",
    "Loader = Load(stuFolder=\"STU\", schFolder='SCH', tchFolder='TCH',\n",
    "                onlyCodeBook = LOAD_ONLY_CODEBOOK,\n",
    "                codeBook=r'C:\\Users\\jhun1\\Dropbox\\[3]Project\\[혼합효과 랜덤포레스트_2022]\\drive-download-20220816T053902Z-001\\PISA2018_CODEBOOK (변수선택-공유).xlsx'\n",
    "                )\n",
    "if LOAD_ONLY_CODEBOOK == False:\n",
    "    Loader.defaultCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loader.default_cleaningData.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(os.path.join(BASE_DIR, 'data', 'cleanedData(SK).xlsx')) as writer:\n",
    "    Loader.default_cleaningData['SK'][0].to_excel(writer, sheet_name='stu', index=False)\n",
    "    Loader.default_cleaningData['SK'][1].to_excel(writer, sheet_name='sch', index=False)\n",
    "    Loader.default_cleaningData['SK'][2].to_excel(writer, sheet_name='tch', index=False) \n",
    "\n",
    "\n",
    "with pd.ExcelWriter(os.path.join(BASE_DIR, 'data', 'cleanedData(US).xlsx')) as writer:\n",
    "    Loader.default_cleaningData['US'][0].to_excel(writer, sheet_name='stu', index=False)\n",
    "    Loader.default_cleaningData['US'][1].to_excel(writer, sheet_name='sch', index=False)\n",
    "    Loader.default_cleaningData['US'][2].to_excel(writer, sheet_name='tch', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞선 처리 데이터 불러오기\n",
    "loadedData = {'SK': [\n",
    "            pd.read_excel(os.path.join(BASE_DIR,'data', 'cleanedData(SK).xlsx'), sheet_name='stu'),\n",
    "            pd.read_excel(os.path.join(BASE_DIR,'data', 'cleanedData(SK).xlsx'), sheet_name='sch'),\n",
    "            pd.read_excel(os.path.join(BASE_DIR,'data', 'cleanedData(SK).xlsx'), sheet_name='tch'),\n",
    "            \n",
    "            ],\n",
    "        'US': [\n",
    "            pd.read_excel(os.path.join(BASE_DIR,'data', 'cleanedData(US).xlsx'), sheet_name='stu'),\n",
    "            pd.read_excel(os.path.join(BASE_DIR,'data', 'cleanedData(US).xlsx'), sheet_name='sch'),\n",
    "            pd.read_excel(os.path.join(BASE_DIR,'data', 'cleanedData(US).xlsx'), sheet_name='tch'),\n",
    "            ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, LoadedData, codeBook, dummyCodeBook):\n",
    "        self.data = LoadedData\n",
    "        self.cb = codeBook\n",
    "        with open(dummyCodeBook, encoding='utf-8') as json_file:\n",
    "            self.dummyCB = json.load(json_file)\n",
    "        self.testBook = {\n",
    "                    'read/math/sci_1': 'PV1MATH PV1READ PV1SCIE'.split(),\n",
    "                    'read/math_1': 'PV1MATH PV1READ'.split(),\n",
    "                    'read/math_10': 'PV1MATH PV2MATH PV3MATH PV4MATH PV5MATH PV6MATH PV7MATH PV8MATH PV9MATH PV10MATH PV1READ PV2READ PV3READ PV4READ PV5READ PV6READ PV7READ PV8READ PV9READ PV10READ'.split(),\n",
    "                    'read_1': ['PV1READ']\n",
    "                }\n",
    "\n",
    "        self._1_dummy = {}\n",
    "        self._2_joined = {}\n",
    "        self._3_dropNa = {}\n",
    "        self._4_ESCS = {'full': {}, 'sliced': {}} # 여기서 데이터 갈라야함\n",
    "        self._5_shouldBeCal = {}\n",
    "        self.finalRS = {}\n",
    "        \n",
    "        self.rs_1_columnFull = pd.DataFrame()\n",
    "        self.rs_1_columnSK = pd.DataFrame()\n",
    "        self.rs_1_columnUS = pd.DataFrame()\n",
    "        \n",
    "\n",
    "    ### Needs1. is selected variable contained in both dataset\n",
    "    def noDataColumn(self): # 열별로 계산\n",
    "        toDrop = {}\n",
    "        \n",
    "        for nationName, nationalData in self.data.items():\n",
    "            print('>> test NA column: ', nationName)\n",
    "\n",
    "            toDrop[nationName] = []\n",
    "            for idx, (label, inputDf) in enumerate(zip('stu sch tch'.split(), nationalData)):\n",
    "                if label == 'tch':\n",
    "                    continue\n",
    "                else:\n",
    "                    print(label)\n",
    "                    for column in inputDf.columns:\n",
    "                        if inputDf[column].isna().sum() > (inputDf.shape[0] * 0.8):\n",
    "                            print('>>> over 80% is NA: ', column)\n",
    "                            toDrop[nationName].append(column)\n",
    "                        \n",
    "                        elif 'missing' in inputDf[column].values:\n",
    "                            print('>>> missing: ', column)\n",
    "                            toDrop[nationName].append(column)\n",
    "                        \n",
    "                        else:\n",
    "                            continue\n",
    "        return toDrop\n",
    "            # assert len(toDrop[nationName]) == 2, print(toDrop)\n",
    "\n",
    "\n",
    "    def Dummy(self, doDummy):\n",
    "        # match key and value from codeBook\n",
    "        print('\\n\\n>>>> 1. Dummy coding')\n",
    "        def matchKV(codeBookDict, inputList):\n",
    "            outputLS = []\n",
    "            for val in inputList:\n",
    "                try:\n",
    "                    outputLS.append(codeBookDict[val])\n",
    "                except KeyError:\n",
    "                    outputLS.append(np.nan)\n",
    "            \n",
    "            return outputLS\n",
    "        \n",
    "        if doDummy == True:\n",
    "            notDummyCol1 = self.cb[self.cb['categories'] == 'identifier'].index\n",
    "            notDummyCol2 = self.cb[self.cb['categories'] == 'resilient status'].index\n",
    "            notDummyCol3 = self.cb[self.cb['file name'] == 'should be caculated'].index\n",
    "            \n",
    "            toDummy = self.cb.drop(list(notDummyCol1)+list(notDummyCol2)+list(notDummyCol3), axis=0) # 더미 변환 안할 변수 행 삭제함\n",
    "            # display(toDummy)\n",
    "\n",
    "            for nationalName, inputNational in self.data.items():\n",
    "                outputNational = copy.deepcopy(inputNational)\n",
    "\n",
    "                for idx, row in toDummy.iterrows(): # 변수별로 반복문\n",
    "                    variable = row['NAME']\n",
    "                    \n",
    "                    if type(row['file name']) != str: # 분석에서 제외할 변수가 있어서 버림\n",
    "                        continue\n",
    "\n",
    "                    else:\n",
    "                        if ('STU' in row['file name']) and (variable in self.dummyCB['stu']):\n",
    "                            outputLS = matchKV(self.dummyCB['stu'][variable], outputNational[0][variable])\n",
    "                            outputNational[0][variable] = outputLS \n",
    "\n",
    "                    # 학교, 교사 데이터는 더미코딩할 것 없음\n",
    "                self._1_dummy[nationalName] = outputNational\n",
    "        elif doDummy == False:\n",
    "            self._1_dummy = copy.deepcopy(self.data)\n",
    "        else:\n",
    "            raise TypeError('>> Error: check option Type')\n",
    "\n",
    "\n",
    "    def Join(self):\n",
    "        print('\\n\\n>>>> 2. Join DataFrame')\n",
    "\n",
    "        for nationalName, inputNational in self._1_dummy.items():\n",
    "            print('>> join nation: ', nationalName)\n",
    "            inputNational[0].reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            outputDf = copy.deepcopy(inputNational[0])\n",
    "            # print('>> before ', outputDf.shape)\n",
    "            before = outputDf.shape\n",
    "\n",
    "            # if sampler.US[1].index.name == 'CNTSCHID':\n",
    "            inputNational[1].drop(['CNTRYID', 'CNT'], axis=1, inplace=True)\n",
    "            if inputNational[1].index.name == 'CNTSCHID':\n",
    "                pass\n",
    "            else:\n",
    "                inputNational[1].set_index('CNTSCHID', drop=True, inplace=True)\n",
    "            \n",
    "            # print(inputNational[1].index)\n",
    "            for idx, row in tqdm(outputDf.iterrows(), desc=\">> mapping\"):\n",
    "                toBeInput = inputNational[1].loc[row['CNTSCHID']].values # 학생 데이터에 들어가야할 학교 데이터 찾기\n",
    "                assert len(toBeInput) == inputNational[1].shape[1]\n",
    "                \n",
    "                toBeInput_T = toBeInput.reshape(1, 8)\n",
    "                outputDf.loc[idx, list(inputNational[1].columns)] = toBeInput_T[0]\n",
    "            \n",
    "            # print('>> after ', outputD>f.shape)\n",
    "            after = outputDf.shape\n",
    "            print('>>>> Bef: ', before, '....', 'Aft: ', after)\n",
    "            assert 'EDUSHORT' in outputDf.columns\n",
    "\n",
    "            self._2_joined[nationalName] = outputDf\n",
    "\n",
    "    def DropStudent(self):\n",
    "        # 각 column 별로 데이터 검수\n",
    "        def column_wise(inputData):\n",
    "            if type(inputData) == dict:\n",
    "                merged = pd.concat([inputData['SK'], inputData['US']])\n",
    "                assert merged.shape[0] == inputData['SK'].shape[0] + inputData['US'].shape[0]\n",
    "            elif type(inputData) == pd.DataFrame:\n",
    "                merged = copy.deepcopy(inputData)\n",
    "            \n",
    "            else:\n",
    "                raise TypeError('>> Error: Check your input D type')\n",
    "                \n",
    "\n",
    "            describeDF = merged.describe().T\n",
    "            describeDF['NA_ratio'] = round(\n",
    "                100 - describeDF['count']/merged.shape[0]*100,\n",
    "                 2\n",
    "                 )\n",
    "\n",
    "            newColumnOrder = [describeDF.columns[0], 'NA_ratio'] + list(describeDF.columns[1:-1])\n",
    "            describeDF= describeDF[newColumnOrder]\n",
    "            return describeDF\n",
    "\n",
    "        #!# 각 학생별로 데이터 검수\n",
    "        def row_wise(inputData):\n",
    "            merged = pd.concat([inputData['SK'], inputData['US']])\n",
    "            assert merged.shape[0] == inputData['SK'].shape[0] + inputData['US'].shape[0]\n",
    "            # unlike column wise, we prepare data with \n",
    "\n",
    "            for_histogram = {}\n",
    "            for label, data in zip(['full', 'SK', 'US'], [merged, inputData['SK'], inputData['US']]):\n",
    "                for_histogram[label] = []\n",
    "\n",
    "                for i in range(len(data.index)) :\n",
    "                    for_histogram[label].append(data.iloc[i].isnull().sum())\n",
    "\n",
    "            fig = plt.figure(figsize=(17,6))\n",
    "\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.hist(for_histogram['full'])\n",
    "            plt.title('\\n전체 데이터\\n')\n",
    "            plt.xlabel('\\n학생별 결측비율\\n')\n",
    "            \n",
    "            \n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.hist(for_histogram['SK'])\n",
    "            plt.title('\\nSouth Korea\\n')\n",
    "            plt.xlabel('\\n학생별 결측비율\\n')\n",
    "            \n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.hist(for_histogram['US'])\n",
    "            plt.title('\\nUnited States\\n')\n",
    "            plt.xlabel('\\n학생별 결측비율\\n')\n",
    "\n",
    "            plt.savefig(os.path.join(BASE_DIR, 'data', f'NA_ratio.jpg'))\n",
    "            plt.show()\n",
    "        \n",
    "        def dropOver(inputData):\n",
    "            output = copy.deepcopy(inputData)\n",
    "            return output\n",
    "        \n",
    "        self.rs_1_columnFull = column_wise(self._2_joined)\n",
    "        self.rs_1_columnSK = column_wise(self._2_joined['SK'])\n",
    "        self.rs_1_columnUS = column_wise(self._2_joined['US'])\n",
    "\n",
    "        row_wise(self._2_joined)\n",
    "        self._3_dropNa = dropOver(self._2_joined)\n",
    "        \n",
    "    def ESCS(self):\n",
    "        #!# 데이터를 두개 만들어야함, result, resultSlice\n",
    "        print('\\n\\n>>>> 4. Slicing data by ESCS')\n",
    "        def escsSlice(inputDict):\n",
    "            assert type(inputDict) == dict, print('>> Error: must input Dict')\n",
    "            output = {'SK': pd.DataFrame(), 'US': pd.DataFrame()}\n",
    "            for nationalName, inputNational in inputDict.items():\n",
    "                slicedData = copy.deepcopy(inputNational)\n",
    "                escsVar = slicedData['ESCS'].quantile(0.25)\n",
    "                \n",
    "                before = slicedData.shape[0]\n",
    "                toDrop = []\n",
    "                for idx, val in enumerate(slicedData['ESCS'].values):\n",
    "                    if val < escsVar:\n",
    "                        continue\n",
    "                    else:\n",
    "                        toDrop.append(idx) # escs 하위 25%를 넘는 친구들은 버림\n",
    "                \n",
    "                slicedData.drop(toDrop, axis=0, inplace=True)\n",
    "                output[nationalName] = slicedData\n",
    "                after = slicedData.shape[0]\n",
    "                print('>> before: ', before, '>> after: ', after)\n",
    "            \n",
    "            return output\n",
    "\n",
    "        def quantileCalculator( \n",
    "                            inputData, # 전체 Full, escs 하위 25%로 데이터셋이 2개로 나뉘므로 인풋을 줘야함\n",
    "                            targetColumn, # 어떻게 변수를 넣어서 테스트할 것인지\n",
    "                            figName, # 그림 제목\n",
    "                            option, # full: 전체 데이터, sliced: 잘린 데이터\n",
    "                            ):\n",
    "            \n",
    "\n",
    "            output = {'SK': pd.DataFrame(), 'US': pd.DataFrame()}\n",
    "            for IDX, (nationalName, inputNational) in enumerate(inputData.items()):\n",
    "                \n",
    "                inputNational['AcademicScore'] = inputNational.loc[:, targetColumn].mean(axis=1)\n",
    "                acadVar = inputNational['AcademicScore'].quantile(0.75) # 상위 25%\n",
    "                if option == 'full':\n",
    "                    escsVar = inputNational['ESCS'].quantile(0.25) # 하위 25%\n",
    "                elif option == 'sliced':\n",
    "                    escsVar = inputNational['ESCS'].quantile(0.0) # 단순히 avgline 그릴 때 방해 안되려고 넣은 것임\n",
    "                total = inputNational.shape[0]\n",
    "                \n",
    "                iamResilient = []\n",
    "                if option == 'full':\n",
    "                    for idx, row in inputNational.iterrows():\n",
    "                        if row['AcademicScore'] > acadVar and row['ESCS'] < escsVar: # sliced 데이터에서는 이 기준을 만족할 수 없음\n",
    "                            iamResilient.append(1)\n",
    "                        else:\n",
    "                            iamResilient.append(0)\n",
    "                elif option == 'sliced':\n",
    "                    for idx, row in inputNational.iterrows():\n",
    "                        if row['AcademicScore'] > acadVar: # sliced 데이터는 escs 기준 필요 없음\n",
    "                            iamResilient.append(1)\n",
    "                        else:\n",
    "                            iamResilient.append(0)\n",
    "\n",
    "                inputNational['resilient'] = iamResilient\n",
    "                resilientCount = [x for x in iamResilient if x ==1]\n",
    "                print(f'\\n>> 회복탄력성 학생수({nationalName}): ', len(resilientCount), f'({round(len(resilientCount)/total*100, 2)})%')\n",
    "                fig = plt.figure(figsize=(17,9))\n",
    "\n",
    "                plt.subplot(2, 2, 2*IDX+1)\n",
    "                plt.hist(inputNational['AcademicScore'])\n",
    "                plt.title(f'\\n학업성취{nationalName}\\n')\n",
    "                plt.xlabel('\\n점수\\n')\n",
    "                plt.axvline(acadVar, color='r', linewidth=1, linestyle='--')\n",
    "                \n",
    "                plt.subplot(2, 2, 2*IDX+2)\n",
    "                plt.hist(inputNational['ESCS'])\n",
    "                plt.title(f'\\n사회문화경제{nationalName}\\n')\n",
    "                plt.xlabel('\\n점수\\n')\n",
    "                plt.axvline(escsVar, color='r', linewidth=1, linestyle='--')\n",
    "\n",
    "                output[nationalName] = inputNational\n",
    "            \n",
    "            plt.savefig(os.path.join(BASE_DIR, 'data', f'{figName}.jpg'))\n",
    "            plt.show()\n",
    "\n",
    "            return output\n",
    "        \n",
    "        self._4_ESCS['full'] = copy.deepcopy(self._3_dropNa) # no drop case, so just copied\n",
    "        self._4_ESCS['sliced'] = escsSlice(self._3_dropNa)\n",
    "\n",
    "        assert type(self._4_ESCS['full']) == dict, print(self._4_ESCS['full'])\n",
    "        # print(self._4_ESCS['full'].keys())\n",
    "\n",
    "        self._4_ESCS['full'] = quantileCalculator(inputData=self._4_ESCS['full'], \n",
    "                                                targetColumn=self.testBook['read_1'], \n",
    "                                                figName ='읽1',\n",
    "                                                option = 'full')\n",
    "        self._4_ESCS['sliced'] = quantileCalculator(inputData=self._4_ESCS['sliced'], \n",
    "                                                targetColumn=self.testBook['read_1'], \n",
    "                                                figName ='읽1(target paper)', \n",
    "                                                option = 'sliced')\n",
    "\n",
    "    \n",
    "    # should be calculated 변수들 계산하는 것임\n",
    "    def shouldBeCalculated(self):\n",
    "        print('\\n\\n>>>> 6. Should Be Calculated')\n",
    "        \n",
    "        def schoolMean(inputDf, whichVar):\n",
    "            assert type(whichVar) == list\n",
    "            outputMean = {}\n",
    "            for sch_id in inputDf['CNTSCHID'].values:\n",
    "                # print('>> ', sch_id)\n",
    "                if sch_id in outputMean.keys():\n",
    "                    continue\n",
    "                \n",
    "                else:\n",
    "                    temp1 = inputDf[inputDf['CNTSCHID'] == sch_id]\n",
    "                    temp2 = temp1.loc[:, whichVar] \n",
    "                    assert len(temp2.columns) == len(whichVar)\n",
    "                    meanVal = np.nanmean(temp2.values)\n",
    "                    assert type(meanVal) == np.float64, print('Error : ', type(meanVal))\n",
    "\n",
    "                    outputMean[sch_id] = meanVal\n",
    "            \n",
    "            return outputMean\n",
    "        \n",
    "        \n",
    "        def meanMapping(inputColumn, mean_dict):\n",
    "            outputLS = []\n",
    "            for idx, sch_id in enumerate(inputColumn.values):\n",
    "                outputLS.append(mean_dict[sch_id])\n",
    "\n",
    "            return outputLS\n",
    "        \n",
    "\n",
    "        def matching(inputData, codeBook):\n",
    "            output = copy.deepcopy(inputData)\n",
    "            shouldBeCal = self.cb[self.cb['file name'] == 'should be caculated']\n",
    "            assert len(shouldBeCal) == 2, print('Error: check self.cb')\n",
    "            for national in output.keys():\n",
    "                beforeShape = output[national].shape[1]\n",
    "                \n",
    "                calVal = list(codeBook[codeBook['categories'] == 'resilient status']['NAME'])\n",
    "                calVal.remove('ESCS')\n",
    "                \n",
    "                for variable in shouldBeCal['NAME'].values:\n",
    "                \n",
    "                    if variable == 'AVG_S_TEST':    \n",
    "                        mean_dict = schoolMean(output[national], calVal)\n",
    "                        # print(f'>> {variable} len: ', len(mean_dict.keys()))\n",
    "                        \n",
    "                    elif variable == 'AVG_S_ESCS':\n",
    "                        mean_dict = schoolMean(output[national], ['ESCS'])\n",
    "                        # print(f'>> {variable} len: ', len(mean_dict.keys()))\n",
    "\n",
    "                    #평균 dict 활용해서 매칭 진행\n",
    "                    outputLS = meanMapping(output[national]['CNTSCHID'], mean_dict)\n",
    "                    assert len(outputLS) == output[national].shape[0], print('Error: ', len(outputLS))\n",
    "                    output[national][variable] = outputLS # 학교 데이터이므로, 학교에 맞춰서 추가하기\n",
    "\n",
    "                afterShape = output[national].shape[1]\n",
    "                assert afterShape - beforeShape == 2, print('Beofre: ', beforeShape, ' ... ', 'After: ', afterShape)\n",
    "            \n",
    "            return output\n",
    "\n",
    "        self._5_shouldBeCal['full'] = matching(self._4_ESCS['full'], codeBook = self.cb)\n",
    "        self._5_shouldBeCal['sliced'] = matching(self._4_ESCS['sliced'], codeBook = self.cb)\n",
    "\n",
    "    \n",
    "    def AdjustMinor(self):\n",
    "    # ```\n",
    "    # 마이너한 것들을 조정하기 위함\n",
    "    # ```\n",
    "        print('\\n\\n>>>> 7. Manually adjust')\n",
    "        # 두 데이터를 나라 row를 만들고, 합치기 위함\n",
    "        def Merge(inputData):\n",
    "            output = pd.concat([inputData['SK'], inputData['US']], axis=0)\n",
    "            assert inputData['SK'].shape[0] + inputData['US'].shape[0] == output.shape[0]\n",
    "\n",
    "            dropAcademic = ['CNTRYID', 'AcademicScore']\n",
    "            for column in output.columns:\n",
    "                if 'PV' in column:\n",
    "                    dropAcademic.append(column)\n",
    "\n",
    "            output.drop(dropAcademic, axis=1, inplace=True)\n",
    "            # print('>> columns: ', output.columns)\n",
    "            return output\n",
    "\n",
    "        # spss 편하도록, 주요 변수들을 앞으로 빼는 작업\n",
    "        def columnOrder(inputData,\n",
    "                        important_columns=['resilient']):\n",
    "            column_ID = ['CNT', 'CNTSCHID', 'CNTSTUID']\n",
    "            # try1\n",
    "            # inputData.set_index(important_columns, inplace=True)\n",
    "            # print('1. ', inputData.columns)\n",
    "\n",
    "            # idxDf = inputData.pop(column_ID)\n",
    "            # inputData.reset_index(inplace=True)\n",
    "            # output = inputData.insert(0, idxDf.name, idxDf)\n",
    "            # print('2. ', output.columns) \n",
    "\n",
    "\n",
    "            inputData.set_index(column_ID+important_columns, inplace=True)\n",
    "            # print(inputData.columns)\n",
    "            inputData.reset_index(inplace=True)\n",
    "            # print(inputData.columns)\n",
    "\n",
    "            return inputData\n",
    "\n",
    "        self.finalRS['full'] = Merge(self._5_shouldBeCal['full'])\n",
    "        self.finalRS['sliced'] = Merge(self._5_shouldBeCal['sliced'])\n",
    "\n",
    "        self.finalRS['full'] = columnOrder(self.finalRS['full'])\n",
    "        self.finalRS['sliced'] = columnOrder(self.finalRS['sliced'])\n",
    "\n",
    "\n",
    "processor = Preprocessing(LoadedData=loadedData, codeBook=Loader.cb, dummyCodeBook='dummyCoding.json')\n",
    "processor.noDataColumn()\n",
    "processor.Dummy(doDummy=False) # 굳이 dummy할 필요가 없음, rf에서 categorical / numerical 인식해야함\n",
    "processor.Join()\n",
    "processor.DropStudent()\n",
    "processor.ESCS()\n",
    "processor.shouldBeCalculated()\n",
    "processor.AdjustMinor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(os.path.join(BASE_DIR, 'data', 'preprocessing.xlsx')) as writer:\n",
    "    processor.finalRS['full'].to_excel(writer, sheet_name='full', index=False)\n",
    "    processor.finalRS['sliced'].to_excel(writer, sheet_name='sliced', index=False)\n",
    "\n",
    "with pd.ExcelWriter(os.path.join(BASE_DIR, 'data', 'descriptive(raw).xlsx')) as writer:\n",
    "    processor.rs_1_columnFull.to_excel(writer, sheet_name='dsec_full', index=True)\n",
    "    processor.rs_1_columnSK.to_excel(writer, sheet_name='dsec_SK', index=True)\n",
    "    processor.rs_1_columnUS.to_excel(writer, sheet_name='dsec_US', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞선 처리 데이터 불러오기\n",
    "cleanedData = pd.read_excel(os.path.join(BASE_DIR, 'data', 'preprocessing.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Descriptive:\n",
    "    def __init__(self, cleanedData):\n",
    "        self.df = cleanedData\n",
    "        self.NUM = {'full': pd.DataFrame(), 'SK': pd.DataFrame(), 'US': pd.DataFrame()}\n",
    "        self.CAT = {'full': pd.DataFrame(), 'SK': pd.DataFrame(), 'US': pd.DataFrame()}\n",
    "\n",
    "        self._1_full = pd.DataFrame()\n",
    "        self._2_SK = pd.DataFrame()\n",
    "        self._3_US = pd.DataFrame()\n",
    "\n",
    "    def Numeric(self):\n",
    "\n",
    "        def naRatio(fullCount, inputArr):\n",
    "            output = 100 - inputArr/fullCount*100\n",
    "            return np.round(output, decimals=2)\n",
    "\n",
    "        def Full(inputData):\n",
    "            output = inputData.describe().T.round(2)\n",
    "            output['na_ratio'] = naRatio(inputData.shape[0], output['count'])\n",
    "            return output\n",
    "\n",
    "        def perNation(inputData):\n",
    "            # print(inputData['CNT'].unique())\n",
    "\n",
    "            output= {}\n",
    "            for nation in inputData['CNT'].unique():\n",
    "                df_nation = inputData[inputData['CNT'] == nation]\n",
    "                output[nation]= df_nation.describe().T.round(2)\n",
    "                output[nation]['na_ratio'] = naRatio(df_nation.shape[0], output[nation]['count'])\n",
    "\n",
    "            return output\n",
    "\n",
    "        self.NUM['full'] = Full(self.df)\n",
    "        nationDict = perNation(self.df)\n",
    "        self.NUM['SK'] = nationDict['Korea']\n",
    "        self.NUM['US'] = nationDict['United States']\n",
    "\n",
    "    def Categorical(self):\n",
    "\n",
    "        def sliceCategorical(inputData,\n",
    "                            referenceNumerical # for assert \n",
    "                            ):   \n",
    "            num_cols = inputData._get_numeric_data().columns\n",
    "            cat_cols = list(set(inputData.columns) - set(num_cols))\n",
    "            \n",
    "            assert list(num_cols) == list(referenceNumerical.index), print(num_cols)\n",
    "\n",
    "            return cat_cols\n",
    "\n",
    "        def Full(inputData, cat_cols):\n",
    "            df_cat = inputData[cat_cols]\n",
    "            temp = df_cat.describe(include='all').T\n",
    "            temp['na_ratio'] = temp['freq'].values/temp['count'].values*100\n",
    "            # temp['na_ratio'] = np.round(temp['na_ratio'], 2) #!# error: infinite\n",
    "            return temp\n",
    "            \n",
    "\n",
    "        def perNation(inputData, cat_cols):\n",
    "            output= {}\n",
    "            for nation in inputData['CNT'].unique():\n",
    "                df_nation = inputData[inputData['CNT'] == nation]\n",
    "                df_cat = df_nation[cat_cols]\n",
    "                temp = df_cat.describe(include='all').T\n",
    "                \n",
    "                temp['na_ratio'] = temp['freq'].values/temp['count'].values*100\n",
    "                # temp['na_ratio'] = np.round(temp['na_ratio'], 2) #!# error: infinite\n",
    "                output[nation] = temp.round(2)\n",
    "                \n",
    "            return output\n",
    "\n",
    "        \n",
    "        cat_cols = sliceCategorical(self.df, referenceNumerical=self.NUM['full'])\n",
    "        self.CAT['full'] = Full(self.df, cat_cols=cat_cols)\n",
    "        nationDict = perNation(self.df, cat_cols=cat_cols)\n",
    "        self.CAT['SK'] = nationDict['Korea']\n",
    "        self.CAT['US'] = nationDict['United States']\n",
    "        \n",
    "    \n",
    "    def Join(self):\n",
    "\n",
    "        def merger(df_num, df_cat):\n",
    "            output = pd.concat([df_num, df_cat], axis=0) # concate on row\n",
    "            # for cat_idx in df_cat.index:\n",
    "            # 합치면 좋겠지만, num, cat 구분 잘되서 좋음\n",
    "\n",
    "            ## drop unnecessary columns\n",
    "            output.drop(columns=['min', '25%', '50%', '75%', 'max'], inplace=True)\n",
    "            output.drop(index=['CNTSCHID', 'CNTSTUID', 'CNT'], inplace=True)\n",
    "\n",
    "            ## re-order columns \n",
    "            output.reset_index(drop=False, inplace=True)\n",
    "            output.set_index(['index', 'count', 'na_ratio'], inplace=True)\n",
    "            output.reset_index(drop=False, inplace=True)\n",
    "            output.set_index('index',inplace=True)\n",
    "            return output.round(2)\n",
    "        \n",
    "        self._1_full = merger(self.NUM['full'], self.CAT['full'])\n",
    "        self._2_SK = merger(self.NUM['SK'], self.CAT['SK'])\n",
    "        self._3_US = merger(self.NUM['US'], self.CAT['US'])\n",
    "\n",
    "        # display(self._1_full)\n",
    "        # display(self._2_SK)\n",
    "        # display(self._3_US)\n",
    "        \n",
    "\n",
    "analyzer = Descriptive(cleanedData)\n",
    "analyzer.Numeric()\n",
    "analyzer.Categorical()\n",
    "analyzer.Join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(os.path.join(BASE_DIR, 'data', 'descriptive(cleaned).xlsx')) as writer:\n",
    "    analyzer._1_full.to_excel(writer, sheet_name='full', index=True)\n",
    "    analyzer._2_SK.to_excel(writer, sheet_name='Korea', index=True)\n",
    "    analyzer._3_US.to_excel(writer, sheet_name='United States', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "- just for comparing results of random forest btw python and r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('SNURO')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5702d5a1f35543d6c34eea9cfe7c421721e3098aad62c19242cc5f6d6a95c445"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
